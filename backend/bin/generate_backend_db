#!/usr/bin/env python3

###############################################################################
#
#    Copyright (C) 2021 Ben Woodcroft
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
###############################################################################

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2020"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft"
__email__ = "benjwoodcroft near gmail.com"
__status__ = "Development"

import argparse
import gzip
import logging
import sys
import os
import shutil
import csv
import subprocess
import json
import datetime
import tempfile

import extern
import pandas as pd
from pytz import country_names
from tqdm import tqdm

sys.path = [os.path.join(os.path.dirname(os.path.realpath(__file__)),'..')] + sys.path
from api.application import create_app

# from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import update, bindparam

from api.models import *

sys.path = ['.'] + sys.path
from sandpiper.parse_biosample_extras import *

from singlem.condense import WordNode
from singlem.otu_table_collection import StreamingOtuTableCollection


def fill_condensed_and_taxonomy_tables(db, db_path, condensed_otu_table_path, blacklist, run_accession_to_id):
    taxonomy_name_to_id = {}
    next_taxonomy_id = 0
    count = 0
    sample_count = 0

    current_sample = None
    current_sample_entries = []
    logging.info("Processing condensed profiles and associated taxonomy tables ..")

    skipped_runs = set()

    with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8') as tf:
        for row in iterate_condensed_profiles(condensed_otu_table_path, blacklist):
            if row[0] not in run_accession_to_id:
                skipped_runs.add(row[0])
                continue

            # Calculate per-sample because we need to calculate relative abundance within-sample
            if row[0] != current_sample:
                if current_sample is not None:
                    (taxonomy_name_to_id, next_taxonomy_id, count) = fill_a_condensed_profile_sample(db, run_accession_to_id, current_sample_entries, taxonomy_name_to_id, next_taxonomy_id, count, tf)
                current_sample = row[0]
                current_sample_entries = []
                sample_count += 1
                if sample_count % 10000 == 0:
                    logging.info("Processed condensed profiles for {} samples".format(sample_count))
            
            current_sample_entries.append(row)
            
        if current_sample is not None:
            (taxonomy_name_to_id, next_taxonomy_id, count) = fill_a_condensed_profile_sample(db, run_accession_to_id, current_sample_entries, taxonomy_name_to_id, next_taxonomy_id, count, tf)

        # Commit taxonomy entries
        logging.info("Committing taxonomy entries from condensed profiles ..")
        db.session.commit()
        
        logging.info("Committing condensed profiles ..")
        tf.flush()
        extern.run("sqlite3 {}".format(db_path), stdin= \
            '.separator "\\t"\n' \
            ".import {} condensed_profiles".format(tf.name))
        logging.info("Wrote {} condensed profile rows to DB, skipping {} that were not in metadata".format(count, len(skipped_runs)))

    return taxonomy_name_to_id

def iterate_condensed_profiles(condensed_otu_table_path, blacklist):
    first = True
    for row in iterate_condensed_profile_reader_gzip_or_not(condensed_otu_table_path):
        if first:
            if len(row) != 3:
                raise Exception("Condensed profile file has unexpected number of columns: {}".format(len(row)))
            first = False
            continue
        if row[0] in blacklist:
            continue
        yield row

def iterate_condensed_profile_reader_gzip_or_not(condensed_otu_table_path):
    if condensed_otu_table_path.endswith('.gz'):
        with gzip.open(condensed_otu_table_path, 'rt') as csvfile_in:
            reader = csv.reader(csvfile_in, delimiter="\t")
            yield from reader
    else:
        with open(condensed_otu_table_path, 'r') as csvfile_in:
            reader = csv.reader(csvfile_in, delimiter="\t")
            yield from reader
    

def fill_a_condensed_profile_sample(db, run_accession_to_id, entries, taxonomy_name_to_id, next_taxonomy_id, count, tf):
    total_coverage = sum([float(x[1]) for x in entries])
    
    # Generate WordNode tree so we can calculate relative abundance later
    root = WordNode(None, 'Root')
    taxons_to_wordnode = {root.word: root}

    inserted_taxonomy_ids = set()
    
    for row, taxons in iterate_condensed_profile_split_taxonomy(entries):
        coverage = float(row[1])
        taxonomy = row[2]
        logging.debug("Analysing taxonomy %s", taxonomy)
        
        last_taxon = root
        wn = None
        for (i, tax) in enumerate(taxons):
            if tax not in taxons_to_wordnode:
                wn = WordNode(last_taxon, tax)
                wn.coverage = 0
                logging.debug("Adding tax %s with prev %s", tax, last_taxon.word)
                last_taxon.children[tax] = wn
                taxons_to_wordnode[tax] = wn #TODO: Problem when there is non-unique labels? Require full taxonomy used?
            last_taxon = taxons_to_wordnode[tax]
        last_taxon.coverage = coverage

    for row, taxons in iterate_condensed_profile_split_taxonomy(entries):
        run_accession = row[0]
        run_id = run_accession_to_id[run_accession]

        # Get the taxonomy id, creating a new entry if necessary
        taxonomy_ids_here = []
        for (i, species_aware_taxonomy) in enumerate(taxons):
            if species_aware_taxonomy not in taxonomy_name_to_id:
                if species_aware_taxonomy == 'Root':
                    level = 'root'
                    parent_id = 'NULL'
                    full_taxonomy = 'Root'
                else:
                    if i==7:
                        level = 'species'
                    else:
                        prefix = species_aware_taxonomy.split('__')[0]
                        level_id = ['d','p','c','o','f','g','s'].index(prefix)
                        level = ['domain','phylum','class','order','family','genus','species'][level_id]
                        # Will throw KeyError if something is amiss

                    parent_id = taxonomy_name_to_id[taxons[i-1]]
                    full_taxonomy = '; '.join(taxons[1:(i+1)])

                db.session.add(Taxonomy(id=next_taxonomy_id, taxonomy_level=level, parent_id=parent_id, name=species_aware_taxonomy, full_name=full_taxonomy))

                taxonomy_name_to_id[species_aware_taxonomy] = next_taxonomy_id
                next_taxonomy_id += 1

            taxonomy_ids_here.append(taxonomy_name_to_id[species_aware_taxonomy])

        def add_a_condensed_profile_taxon(count, taxon_name, taxon_level_index, tf): # db, , taxons_to_wordnode, taxonomy, taxonomy_ids_here, taxonomy_name_to_id
            # condensed_profile_entry = {
            #     'id': count+1,
            #     'run_id': run_id,
            #     'coverage': taxons_to_wordnode[taxon_name].coverage,
            #     'filled_coverage': taxons_to_wordnode[taxon_name].get_full_coverage(),
            #     'relative_abundance': taxons_to_wordnode[taxon_name].get_full_coverage() / total_coverage,
            #     'taxonomy_id': taxonomy_name_to_id[taxon_name]
            # }
            # for (idx, column_name) in zip(taxonomy_ids_here[:(taxon_level_index+1)], Taxonomy.taxonomy_level_columns):
            #     condensed_profile_entry[column_name] = idx
            # condensed_profile_entry = {
            #     'id': count+1,
            #     'run_id': run_id,
            #     'coverage': taxons_to_wordnode[taxon_name].coverage,
            #     'filled_coverage': taxons_to_wordnode[taxon_name].get_full_coverage(),
            #     'relative_abundance': taxons_to_wordnode[taxon_name].get_full_coverage() / total_coverage,
            #     'taxonomy_id': taxonomy_name_to_id[taxon_name]
            # }
            # for (idx, column_name) in zip(taxonomy_ids_here[:(taxon_level_index+1)], Taxonomy.taxonomy_level_columns):
            #     condensed_profile_entry[column_name] = idx
            # db.session.add(CondensedProfile(**condensed_profile_entry))
            # if count % 10000 == 0:
            #     db.session.commit()

            # id|run_id|coverage|filled_coverage|relative_abundance|taxonomy_id|domain_id|phylum_id|class_id|order_id|family_id|genus_id|species_id
            to_print = [
                count+1,
                run_id,
                taxons_to_wordnode[taxon_name].coverage,
                taxons_to_wordnode[taxon_name].get_full_coverage(),
                taxons_to_wordnode[taxon_name].get_full_coverage() / total_coverage,
                taxonomy_name_to_id[taxon_name],
            ]
            for (idx, column_name) in zip(taxonomy_ids_here[:(taxon_level_index+1)], Taxonomy.taxonomy_level_columns):
                to_print.append(idx)
            print('\t'.join(map(str, to_print)), file=tf)

            count += 1
            return count

        # In some cases, like c__Coriobacteriia in the 6_runs test dataset,
        # there is no actual entry for c__Coriobacteriia in the condensed
        # profile. So that searching condensed profiles for taxonomy returns
        # results for c__Coriobacteriia, we need to add an inferred entry.
        for (i, taxon) in enumerate(taxons):
            if taxonomy_name_to_id[taxon] not in inserted_taxonomy_ids:
                count = add_a_condensed_profile_taxon(count, taxon, i, tf)
                inserted_taxonomy_ids.add(taxonomy_name_to_id[taxon])

    return (taxonomy_name_to_id, next_taxonomy_id, count)

def iterate_condensed_profile_split_taxonomy(entries):
    for row in entries:
        taxonomies = row[2]
        taxons_split = taxonomies.split('; ')
        species_aware_taxonomies = []

        for (i, taxonomies_split) in enumerate(taxons_split):
            # Store species as species+genus since species aren't unique. But this already happens, so don't need to worry
            species_aware_taxonomy = taxonomies_split.strip()
            species_aware_taxonomies.append(species_aware_taxonomy)

        yield row, species_aware_taxonomies

def create_sample_metadata(db, condensed_otu_table_path, blacklist, metadata_json_files, corrections):
    '''Load metadata including directly parsed metadata, using only samples in the condensed otu table'''

    # Only load metadata for samples that are in the DB
    samples_of_interest = set([x[0] for x in iterate_condensed_profiles(condensed_otu_table_path, blacklist)])
    logging.info("Finding metadata for {} samples".format(len(samples_of_interest)))
    
    last_metadata_id = 0
    attribute_count = 0
    run_accession_to_id = {}
    num_corrections = 0
    for path in metadata_json_files:
        with open(path) as f:
            for line in f:
                j = json.loads(line)

                acc = j['acc']
                if acc not in samples_of_interest:
                    logging.debug("Skipping {}".format(acc))
                    continue

                # Some odd (old?) runs do not have an associated bioproject e.g. SRR2001402
                if ('bioproject' in j and j['bioproject'] in corrections) or acc in corrections:
                    if acc in corrections:
                        items_here = corrections[acc].items()
                    else:
                        items_here = corrections[j['bioproject']].items()
                    for (field, correction) in items_here:
                        num_corrections += 1
                        if field in j:
                            if (type(correction['old_value']) == str and j[field] == correction['old_value']) or \
                                (type(correction['old_value']) == list and j[field] in correction['old_value']):
                                j[field] = correction['corrected_value']
                                logging.info("Correcting {}: '{}' from '{}' to '{}'".format(acc, field, correction['old_value'], correction['corrected_value']))
                            else:
                                raise Exception("For acc {} Did not find expected old value {} in {}".format(acc, correction['old_value'], j[field]))
                        elif field in [kv['k'] for kv in j['attributes']]:
                            for i, kv in enumerate(j['attributes']):
                                if kv['k'] == field:
                                    if (type(correction['old_value']) == str and kv['v'] == correction['old_value']) or \
                                        (type(correction['old_value']) == list and kv['v'] in correction['old_value']):
                                        j['attributes'][i]['v'] = correction['corrected_value']
                                        logging.info("Correcting {}: '{}' from '{}' to '{}'".format(acc, field, correction['old_value'], correction['corrected_value']))
                                    else:
                                        raise Exception("For acc {} Did not find expected old value '{}' in '{}'".format(acc, correction['old_value'], kv['v']))
                        else:
                            raise Exception("Unexpected correction field encountered: {}".format(field))


                attributes = j['attributes']

                del j['attributes']
                # Convert datetimes
                # releasedate = db.Column(db.DateTime, nullable=False)
                # loaddate = db.Column(db.DateTime, nullable=False)
                # collection_date_sam = db.Column(db.DateTime, nullable=False)
                for d in ['releasedate','loaddate','collection_date_sam']:
                    if d in j and j[d] is not None and j[d].lower() not in ACTUALLY_MISSING:
                        try:
                            j[d] = iso8601.parse_date(j[d])
                        except iso8601.ParseError:
                            logging.warning("Unexpected date value for %s: %s" % (acc, j[d]))

                del j['biosamplemodel_sam'] #Assume it is defined in the associated sample metadata

                # Unclear to me why these exist as arrays rather than single values, but eh
                for d in ['geo_loc_name_sam','ena_first_public_run','ena_last_update_run','sample_name_sam']:
                    if d in j:
                        if len(j[d]) > 1:
                            logging.warning("Unexpectedly found a metadata array for {} in {}: {}, just taking the first one".format(d, acc, j[d]))
                            j[d] = j[d][0]
                        try:
                            j[d] = j[d][0]
                        except IndexError:
                            del j[d]
                meta = NcbiMetadata(**j)
                db.session.add(meta)
                last_metadata_id += 1
                run_accession_to_id[acc] = last_metadata_id

                # Write biosample attributes
                for attribute in attributes:
                    # Don't record bases/bytes as these aren't really biosample attributes
                    if attribute['k'] not in ('bases','bytes'):
                        attribute['run_id'] = last_metadata_id
                        db.session.add(BiosampleAttribute(**attribute))
                        attribute_count += 1

                # Parsed data
                j['attributes'] = attributes
                lat_lon = parse_lat_lons(j['acc'], parse_json_to_lat_lon_dict(j))
                # logging.info("Parsed lat_lon to {}".format(lat_lon))
                collection_year, collection_month = parse_extra_sample_attributes(j, ['collection_date_sam'])
                if collection_year == '': collection_year = None
                if collection_month == '': collection_month = None
                depth = parse_extra_sample_attributes(j, ['depth_sam'])[0]
                if depth == '': depth = None
                temperature = parse_extra_sample_attributes(j, ['temperature_sam'])[0]
                if temperature == '': temperature = None
                db.session.add(ParsedSampleAttribute(**{
                    'run_id': last_metadata_id,
                    'collection_year': collection_year,
                    'collection_month': collection_month,
                    'depth': depth,
                    'temperature': temperature,
                    'latitude': lat_lon[0],
                    'longitude': lat_lon[1]
                }))

                if last_metadata_id % 10000 == 0:
                    db.session.commit()

    db.session.commit()
    logging.info("Wrote {} metadata and {} attribute rows to DB, with {} corrections".format(last_metadata_id, attribute_count, num_corrections))

    return run_accession_to_id

def add_metadata_from_kingfisher_annotate(db, run_accession_to_id, kingfisher_annotate_path):
    '''Add metadata from kingfisher_annotate.json'''

    num_kingfisher_annotated = 0
    num_study_links = 0

    # On
    # sandpiper/sra_metadata/sra_metadata_20220524_kingfisher/sra_metadata_20220524.annotated315.tsv
    # was getting a buffer overflow error with the default engine
    df = pd.read_csv(kingfisher_annotate_path, sep='\t', header=0, engine='python')

    # experiment_title = db.Column(db.String)
    # library_strategy = db.Column(db.String)
    # instrument_model = db.Column(db.String) # model column in kingfisher
    # organisation_department = db.Column(db.String)
    # organisation_institution = db.Column(db.String)
    # organisation_street = db.Column(db.String)
    # organisation_city = db.Column(db.String)
    # organisation_country = db.Column(db.String)
    # organisation_contact_name = db.Column(db.String)
    # study_title = db.Column(db.String)
    # study_abstract = db.Column(db.String)
    update_stmt = (
        update(NcbiMetadata).
        where(NcbiMetadata.id == bindparam('run_id')).
        values(
            experiment_title = bindparam('experiment_title'), #row.experiment_title,
            library_strategy = bindparam('library_strategy'), #row.library_strategy,
            instrument_model = bindparam('model'), #row.model,
            # # meta.organisation_department = row.organisation_department
            # # meta.organisation_institution = row.organisation_institution
            # # meta.organisation_street = row.organisation_street
            # # meta.organisation_city = row.organisation_city
            # # meta.organisation_country = row.organisation_country
            # # meta.organisation_contact_name = row.organisation_contact_name
            study_title = bindparam('study_title'), #row.study_title,
            study_abstract = bindparam('study_abstract'), #row.study_abstract,
            design_description = bindparam('design_description'), #row.design_description
            read1_length_average = bindparam('read1_length_average'), #row.read1_length_average
            read1_length_stdev = bindparam('read1_length_stdev'), #row.read1_length_stdev
            read2_length_average = bindparam('read2_length_average'), #row.read2_length_average
            read2_length_stdev = bindparam('read2_length_stdev'), #row.read2_length_stdev
            )
    )
    update_bindparams = []

    for row in df.itertuples():
        # Find associated NcbiMetadata record
        if row.run not in run_accession_to_id:
            # logging.warning("Skipping {} as there was no condense data for it".format(row.run))
            continue
        run_id = run_accession_to_id[row.run]

        update_bindparams.append({
            'run_id': run_id,
            'experiment_title': row.experiment_title,
            'library_strategy': row.library_strategy,
            'model': row.model,
            'organisation_name': row.organisation,
            'organisation_department': row.organisation_department,
            'organisation_institution': row.organisation_institution,
            'organisation_street': row.organisation_street,
            'organisation_city': row.organisation_city,
            'organisation_country': row.organisation_country,
            'organisation_contact_name': row.organisation_contact_name,
            'study_title': row.study_title,
            'study_abstract': row.study_abstract,
            'design_description': row.design_description,
            'read1_length_average': float(row.read1_length_average) if row.read1_length_average else None,
            'read1_length_stdev': float(row.read1_length_stdev) if row.read1_length_stdev else None,
            'read2_length_average': float(row.read2_length_average) if row.read2_length_average else None,
            'read2_length_stdev': float(row.read2_length_stdev) if row.read2_length_stdev else None,
        })

        for h in json.loads(row.study_links):
            if 'db' in h and 'id' in h:
                db.session.add(StudyLink(run_id=run_id, study_id=h['id'], database=h['db']))
            elif 'label' in h and 'url':
                db.session.add(StudyLink(run_id=run_id, label=h['label'], url=h['url']))
            else:
                raise Exception("Unexpected study link format: {}".format(h))
            num_study_links += 1

        num_kingfisher_annotated += 1

        if num_kingfisher_annotated % 10000 == 0:
            db.session.execute(update_stmt, update_bindparams)
            db.session.commit()
            bindparams = []

    if len(update_bindparams) > 0:
        db.session.execute(update_stmt, update_bindparams)
    db.session.commit()
    logging.info("Wrote {} metadata and {} study link rows to DB".format(num_kingfisher_annotated, num_study_links))

def add_host_predictions(db, run_accession_to_id, host_predictions_tsv):
    num_host_predicted = 0

    df = pd.read_csv(host_predictions_tsv, sep='\t')
    # acc     prediction      organism        host_or_not
    # DRR001356       host    human metagenome        host
    # DRR001360       ecological      human metagenome        host
    # DRR001455       ecological      soil metagenome ecological

    update_stmt = (
        update(ParsedSampleAttribute).
        where(ParsedSampleAttribute.id == bindparam('b_run_id')).
        values(
            host_or_not_prediction = bindparam('host_or_not_prediction'),
            host_or_not_recorded = bindparam('host_or_not_recorded'),
            host_or_not_mature = bindparam('host_or_not_mature'),
            )
    )

    update_bindparams = []

    for row in df.itertuples():
        # Find associated NcbiMetadata record
        if row.acc not in run_accession_to_id:
            logging.debug("Skipping {} as there was no condense data for it".format(row.acc))
            continue
        run_id = run_accession_to_id[row.acc]

        update_bindparams.append({
            'b_run_id': run_id,
            'host_or_not_prediction': row.prediction,
            'host_or_not_recorded': row.host_or_not,
            'host_or_not_mature': row.prediction if str(row.host_or_not)=='nan' else row.host_or_not
        })

        num_host_predicted += 1

        if num_host_predicted % 10000 == 0:
            db.session.execute(update_stmt, update_bindparams)
            db.session.commit()
            bindparams = []

    db.session.execute(update_stmt, update_bindparams)
    db.session.commit()
    logging.info("Wrote {} host predictions to DB".format(num_host_predicted))


def index_otu_table(db, otu_table_file, run_accession_to_id, taxonomy_name_to_id, db_path):
    # For each OTU in the input, create a new line in the otus_indexed table,
    # which is supposed to have better query performance when searhcing for
    # specific samples.
    logging.info("Indexing OTU table")

    count = 0

    otus = StreamingOtuTableCollection()
    otus.add_otu_table_file(otu_table_file)

    # Get a list of the marker IDs
    marker_name_to_id = {}
    for marker in Marker.query.all():
        marker_name_to_id[marker.marker] = marker.id
    logging.info("Loaded {} markers".format(len(marker_name_to_id)))

    next_taxonomy_id = Taxonomy.query.count() + 1
    logging.info("Next taxonomy ID is {}".format(next_taxonomy_id))

    # Put Eukaryotes in as a special case, as these are never in the condensed
    # profile.
    if 'd__Eukaryota' not in taxonomy_name_to_id: # which it currently never is
        taxon = 'd__Eukaryota'

        prefix = taxon.split('__')[0]
        level_id = ['d','p','c','o','f','g','s'].index(prefix)
        level = ['domain','phylum','class','order','family','genus','species'][level_id]

        db.session.add(Taxonomy(
            id=next_taxonomy_id,
            taxonomy_level=level,
            parent_id=Taxonomy.query.filter_by(taxonomy_level='root').first().id,
            name=taxon,
            full_name=taxon))

        taxonomy_name_to_id[taxon] = next_taxonomy_id
        next_taxonomy_id += 1
    db.session.commit()

    with tempfile.TemporaryDirectory() as my_tempdir:
        # my_tempdir = '/tmp/'
        with open(os.path.join(my_tempdir, 'otus_indexed.tsv'), 'w') as f:
            for otu in tqdm(otus):
                # Do not add samples where the sample is not in the rest of the DB
                if otu.sample_name in run_accession_to_id:
                    split_taxonomy = otu.taxonomy.split('; ')
                    try:
                        taxonomy_name_to_id[split_taxonomy[-1]]
                    except KeyError:
                        # Taxonomy didn't make it into any condensed profile, so hasn't been established.
                        i = len(split_taxonomy) - 1
                        taxons_to_add = []
                        while i >= 0:
                            if split_taxonomy[i] in taxonomy_name_to_id:
                                parent_id = taxonomy_name_to_id[split_taxonomy[i]]
                                break
                            taxons_to_add.append(split_taxonomy[i])
                            i -= 1
                        if i == 0:
                            raise Exception("Strangely didn't find any taxonomy in the table for {}".format(otu.taxonomy))
                        taxons_to_add = reversed(taxons_to_add)
                        
                        full_taxonomy = split_taxonomy[1:(i+1)] # skip first element as that is Root
                        for taxon in taxons_to_add:
                            if split_taxonomy[1] == 'd__Eukaryota':
                                level_id = 0
                                level = 'phylum' # Not quite true but doesn't matter
                            else:
                                prefix = taxon.split('__')[0]
                                level_id = ['d','p','c','o','f','g','s'].index(prefix)
                                level = ['domain','phylum','class','order','family','genus','species'][level_id]
                            
                            # Insert the new taxonomy into the DB
                            full_taxonomy.append(taxon)
                            logging.debug("Adding full name {}".format(full_taxonomy))
                            db.session.add(Taxonomy(
                                id=next_taxonomy_id,
                                taxonomy_level=level,
                                parent_id=parent_id,
                                name=taxon,
                                full_name='; '.join(full_taxonomy)))

                            taxonomy_name_to_id[taxon] = next_taxonomy_id
                            parent_id = next_taxonomy_id
                            next_taxonomy_id += 1

                    f.write("\t".join([str(x) for x in [
                        count + 1,
                        run_accession_to_id[otu.sample_name],
                        otu.count,
                        otu.coverage,
                        taxonomy_name_to_id[split_taxonomy[-1]],
                        marker_name_to_id[otu.marker],
                        otu.sequence
                    ]])+"\n")
                    count += 1

        logging.info("Committing taxonomy entries seen only in OTU table ..")
        db.session.commit()
    
        logging.info("Importing {} OTU lines into DB".format(count))
        extern.run("sqlite3 {}".format(db_path), stdin= \
            '.separator "\\t"\n' \
            ".import {} otus_indexed".format(f.name))
        logging.info("Imported {} OTU lines into DB".format(OtuIndexed.query.count()))

    # drop the otus table since we don't need it anymore
    logging.info("Dropping old otus table ..")
    Otu.__table__.drop(db.session.connection())

def read_corrections():
    with open(os.path.join(os.path.dirname(__file__), '../../public_sequencing_metadata_corrections/corrections.json')) as f:
        corrections = json.load(f)
    # Convert corrections to a dict with key of bioproject
    corrections_by_key = {}
    for correction in corrections:
        # Corrections should be dict of table name to correction info
        table_corrections = {}
        for cor in correction['corrections']:
            table_corrections[cor['field']] = cor
        if 'run' in correction and 'bioproject' in correction:
            raise Exception("Cannot specify both run and bioproject in corrections.json")
        if 'run' in correction:
            key = correction['run']
        elif 'bioproject' in correction:
            key = correction['bioproject']
        else:
            raise Exception("No project or run found in correction {}".format(correction))
        corrections_by_key[key] = table_corrections

    logging.debug("corrections_by_key:", corrections_by_key)
    return corrections_by_key

def add_host_prediction_counts_to_taxonomy(db, db_path):
    with tempfile.NamedTemporaryFile(prefix='sandpiper_generate_taxonomy') as f:
        tax_count = db.session.query(Taxonomy).count()
        logging.info("Caching {} host sample counts in taxonomy ..".format(tax_count))
        for taxonomy in tqdm(Taxonomy.query.all(), total=tax_count):
            num_host_runs = NcbiMetadata.query. \
                join(NcbiMetadata.condensed_profiles).filter_by(taxonomy_id=taxonomy.id). \
                join(NcbiMetadata.parsed_sample_attributes).filter_by(host_or_not_mature='host'). \
                count()
            num_ecological_runs = NcbiMetadata.query. \
                join(NcbiMetadata.condensed_profiles).filter_by(taxonomy_id=taxonomy.id). \
                join(NcbiMetadata.parsed_sample_attributes).filter_by(host_or_not_mature='ecological'). \
                count()
                
            # id = db.Column(db.Integer, primary_key=True)
            # taxonomy_level = db.Column(db.String, nullable=False)
            # parent_id = db.Column(db.Integer, db.ForeignKey('taxonomies.id'), nullable=False)
            # name = db.Column(db.String, nullable=False, index=True)
            # full_name = db.Column(db.String, nullable=False)
            # host_sample_count = db.Column(db.Integer)
            # ecological_sample_count = db.Column(db.Integer)
            f.write(("\t".join([str(x) for x in [
                taxonomy.id,
                taxonomy.taxonomy_level,
                taxonomy.parent_id,
                taxonomy.name,
                taxonomy.full_name,
                num_host_runs,
                num_ecological_runs
            ]])+"\n").encode())
        f.flush()
        extern.run("sqlite3 {}".format(db_path), stdin= \
            'delete from taxonomies;\n' \
            '.separator "\\t"\n' \
            ".import {} taxonomies".format(f.name))
        logging.info("Imported {} taxonomies with host prediction counts".format(Taxonomy.query.count()))

def add_bioproject_associated_publications(db, tsv_path):
    df = pd.read_csv(tsv_path, sep='\t')
    logging.info("Adding {} bioproject associated publications to DB".format(df.shape[0]))
    count_added = 0
    for i, row in tqdm(df.iterrows(), total=df.shape[0]):
        runs = NcbiMetadata.query.filter_by(bioproject=row['bioproject']).all()
        for run in runs:
            current_study_links = StudyLink.query.filter_by(run_id=run.id).all()
            already_there = False
            # Remove duplicate links that are already recorded e.g. SRR9113719
            for link in current_study_links:
                if link.database is not None and link.database.lower() == 'pubmed' and link.study_id == str(row['id']):
                    already_there = True
            if not already_there:
                db.session.add(StudyLink(
                    run_id=run.id,
                    database=row['db'],
                    study_id=row['id']))
                count_added += 1
    db.session.commit()
    logging.info("Added {} bioproject associated publications to DB".format(count_added))

if __name__ == '__main__':
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument('--debug', help='output debug information', action="store_true")
    #parent_parser.add_argument('--version', help='output version information and quit',  action='version', version=repeatm.__version__)
    parent_parser.add_argument('--quiet', help='only output errors', action="store_true")

    parent_parser.add_argument('--singlem-db', help='sqlite file from a singlem db', required=True)
    parent_parser.add_argument('--otu-table', help='OTU table of the singlem db', required=True) # Commented out for now since unsure it will be used.
    parent_parser.add_argument('--condensed-otu-table', help='condensed OTU table input', required=True)
    parent_parser.add_argument('-o', help='sqlite3 file to generate', required=True)
    parent_parser.add_argument('--metadata-json-files', nargs='+', help='metadata json files', required=True)
    parent_parser.add_argument('--run-blacklist', help='Ignore runs listed in this file')
    parent_parser.add_argument('--kingfisher-annotate-path', help='kingfisher annotate TSV output')
    parent_parser.add_argument('--kingfisher-annotate-path-list', help='kingfisher annotate TSV outputs listed in this file, one per line')
    parent_parser.add_argument('--host-predictions-tsv', help='host predictions TSV output', required=True)
    parent_parser.add_argument('--bioproject-associated-publications', help='tsv from bioprjoect_metadata.py', required=True)

    args = parent_parser.parse_args()

    if not args.kingfisher_annotate_path and not args.kingfisher_annotate_path_list:
        raise Exception("Kingfisher annotate data must be specified somehow")

    # Setup logging
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    # Read corrections in
    corrections = read_corrections()

    blacklist = set()
    if args.run_blacklist:
        with open(args.run_blacklist) as f:
            for line in f:
                if line != 'acc':
                    blacklist.add(line.strip())
        logging.info("Ignoring {} runs".format(len(blacklist)))

    # Copy file over
    logging.info("Copying SQLite file")
    sqlite_db_path = args.o
    shutil.copy(args.singlem_db, sqlite_db_path)

    # Create the app and create the tables
    app = create_app()
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + sqlite_db_path
    app.config['SQLALCHEMY_ECHO'] = args.debug is True
    db = SQLAlchemy(app)
    with app.app_context():
        db.create_all()

        # Create all the tables
        Taxonomy.metadata.create_all(db.engine)

        # Create sample metadata table, restricting it to just those samples
        # that we have condensed profiles for.
        run_accession_to_id = create_sample_metadata(db, args.condensed_otu_table, blacklist, args.metadata_json_files, corrections)
        
        taxonomy_name_to_id = fill_condensed_and_taxonomy_tables(db, sqlite_db_path, args.condensed_otu_table, blacklist, run_accession_to_id)

        if args.kingfisher_annotate_path:
            add_metadata_from_kingfisher_annotate(db, run_accession_to_id, args.kingfisher_annotate_path)
        if args.kingfisher_annotate_path_list:
            with open(args.kingfisher_annotate_path_list) as f:
                for line in f:
                    file_path = line.strip()
                    logging.info("Adding metadata from {} ..".format(file_path))
                    add_metadata_from_kingfisher_annotate(db, run_accession_to_id, file_path)

        add_bioproject_associated_publications(db, args.bioproject_associated_publications)

        add_host_predictions(db, run_accession_to_id, args.host_predictions_tsv)

        index_otu_table(db, args.otu_table, run_accession_to_id, taxonomy_name_to_id, sqlite_db_path)

        add_host_prediction_counts_to_taxonomy(db, sqlite_db_path)