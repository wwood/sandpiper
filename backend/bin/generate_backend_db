#!/usr/bin/env python3

###############################################################################
#
#    Copyright (C) 2021 Ben Woodcroft
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
###############################################################################

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2020"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft"
__email__ = "benjwoodcroft near gmail.com"
__status__ = "Development"

import argparse
import logging
import sys
import os
import shutil
import csv
import subprocess
import tempfile
import json
import datetime

import extern

sys.path = [os.path.join(os.path.dirname(os.path.realpath(__file__)),'..')] + sys.path
from api.application import create_app

# from flask import Flask
from flask_sqlalchemy import SQLAlchemy

from api.models import CondensedProfile, NcbiMetadata, Taxonomy, BiosampleAttribute

from singlem.condense import WordNode


def fill_condensed_and_taxonomy_tables(db, condensed_otu_table_path):
    taxonomy_name_to_id = {}
    next_taxonomy_id = 0
    count = 0
    sample_count = 0

    current_sample = None
    current_sample_entries = []
    logging.info("Processing condensed profiles and associated taxonomy tables ..")

    with open(condensed_otu_table_path, 'r') as csvfile_in:
        reader = csv.reader(csvfile_in, delimiter="\t")

        first = True
        for row in reader:
            if first:
                first = False
                continue

            # Calculate per-sample because we need to calculate relative abundance within-sample
            if row[0] != current_sample:
                if current_sample is not None:
                    (taxonomy_name_to_id, next_taxonomy_id, count) = fill_a_condensed_profile_sample(db, row[0], current_sample_entries, taxonomy_name_to_id, next_taxonomy_id, count)
                current_sample = row[0]
                current_sample_entries = []
                sample_count += 1
                if sample_count % 10000 == 0:
                    logging.info("Processed condensed profiles for {} samples".format(sample_count))
            
            current_sample_entries.append(row)
        
    if current_sample is not None:
        (taxonomy_name_to_id, next_taxonomy_id, count) = fill_a_condensed_profile_sample(db, row[0], current_sample_entries, taxonomy_name_to_id, next_taxonomy_id, count)

    db.session.commit()
    logging.info("Wrote {} condensed profile rows to DB".format(count))
    

def fill_a_condensed_profile_sample(db, sample_name, entries, taxonomy_name_to_id, next_taxonomy_id, count):
    total_coverage = sum([float(x[1]) for x in entries])
    
    # Generate WordNode tree so we can calculate relative abundance later
    root = WordNode(None, 'Root')
    taxons_to_wordnode = {root.word: root}
    
    for row, taxons in iterate_condensed_profile_split_taxonomy(entries):
        coverage = float(row[1])
        taxonomy = row[2]
        logging.debug("Analysing taxonomy %s", taxonomy)
        
        last_taxon = root
        wn = None
        for (i, tax) in enumerate(taxons):
            if tax not in taxons_to_wordnode:
                wn = WordNode(last_taxon, tax)
                logging.debug("Adding tax %s with prev %s", tax, last_taxon.word)
                last_taxon.children[tax] = wn
                taxons_to_wordnode[tax] = wn #TODO: Problem when there is non-unique labels? Require full taxonomy used?
            last_taxon = taxons_to_wordnode[tax]
        last_taxon.coverage = coverage

    for row, taxons in iterate_condensed_profile_split_taxonomy(entries):
        # Get the taxonomy id, creating a new entry if necessary
        taxonomy_ids_here = []
        for (i, species_aware_taxonomy) in enumerate(taxons):
            if species_aware_taxonomy not in taxonomy_name_to_id:
                if species_aware_taxonomy == 'Root':
                    level = 'root'
                    parent_id = 'NULL'
                    full_taxonomy = 'Root'
                else:
                    if i==7:
                        level = 'species'
                    else:
                        prefix = species_aware_taxonomy.split('__')[0]
                        level_id = ['d','p','c','o','f','g','s'].index(prefix)
                        level = ['domain','phylum','class','order','family','genus','species'][level_id]
                        # Will throw KeyError if something is amiss

                    parent_id = taxonomy_name_to_id[taxons[i-1]]
                    full_taxonomy = '; '.join(taxons[1:])

                db.session.add(Taxonomy(id=next_taxonomy_id, taxonomy_level=level, parent_id=parent_id, name=species_aware_taxonomy, full_name=full_taxonomy))

                taxonomy_name_to_id[species_aware_taxonomy] = next_taxonomy_id
                next_taxonomy_id += 1

            taxonomy_ids_here.append(taxonomy_name_to_id[species_aware_taxonomy])

        condensed_profile_entry = {
            'id': count+1,
            'sample_name': row[0],
            'coverage': row[1],
            'filled_coverage': taxons_to_wordnode[taxons[-1]].get_full_coverage(),
            'relative_abundance': taxons_to_wordnode[taxons[-1]].get_full_coverage() / total_coverage,
            'taxonomy_id': taxonomy_name_to_id[species_aware_taxonomy]
        }
        for (idx, column_name) in zip(taxonomy_ids_here, Taxonomy.taxonomy_level_columns):
            condensed_profile_entry[column_name] = idx

        db.session.add(CondensedProfile(**condensed_profile_entry))
        count += 1

        if count % 10000 == 0:
            db.session.commit()

    return (taxonomy_name_to_id, next_taxonomy_id, count)

def iterate_condensed_profile_split_taxonomy(entries):
    for row in entries:
        taxonomies = row[2]
        taxons_split = taxonomies.split('; ')
        species_aware_taxonomies = []

        for (i, taxonomies_split) in enumerate(taxons_split):
            # Store species as species+genus since species aren't unique. But this already happens, so don't need to worry
            species_aware_taxonomy = taxonomies_split.strip()
            species_aware_taxonomies.append(species_aware_taxonomy)

        yield row, species_aware_taxonomies

def fill_metadata(db, metadata_json_files):
    # Load metadata
    # Only load metadata for samples that are in the DB
    samples_of_interest = set([x[0] for x in CondensedProfile.query.with_entities(CondensedProfile.sample_name).distinct()])
    logging.info("Finding metadata for {} samples".format(len(samples_of_interest)))
    
    metadata_count = 0
    attribute_count = 0
    for path in metadata_json_files:
        with open(path) as f:
            for line in f:
                j = json.loads(line)
                acc = j['acc']
                if acc not in samples_of_interest:
                    logging.debug("Skipping {}".format(acc))
                    continue
                attributes = j['attributes']

                del j['attributes']
                # Convert datetimes
                # releasedate = db.Column(db.DateTime, nullable=False)
                # loaddate = db.Column(db.DateTime, nullable=False)
                # collection_date_sam = db.Column(db.DateTime, nullable=False)
                for d in ['releasedate','loaddate']:
                    if d in j:
                        j[d] = datetime.datetime.strptime(j[d], '%Y-%m-%dT%H:%M:%S+00:00')
                for d in ['collection_date_sam']:
                    if d in j:
                        j[d] = datetime.datetime.strptime(j[d], '%Y-%m-%d')

                del j['biosamplemodel_sam'] #Assume it is defined in the associated sample metadata

                # Unclear to me why these exist as arrays rather than single values, but eh
                for d in ['geo_loc_name_sam','ena_first_public_run','ena_last_update_run','sample_name_sam']:
                    if d in j:
                        if len(j[d]) > 1:
                            logging.warning("Unexpectedly found a metadata array for {} in {}: {}, just taking the first one".format(d, acc, j[d]))
                            j[d] = j[d][0]
                        try:
                            j[d] = j[d][0]
                        except IndexError:
                            del j[d]
                meta = NcbiMetadata(**j)
                db.session.add(meta)
                metadata_count += 1

                # Write biosample attributes
                for attribute in attributes:
                    attribute['run_id'] = metadata_count
                    db.session.add(BiosampleAttribute(**attribute))
                    attribute_count += 1

                if metadata_count % 10000 == 0:
                    db.session.commit()

    db.session.commit()
    logging.info("Wrote {} metadata and {} attribute rows to DB".format(metadata_count, attribute_count))

if __name__ == '__main__':
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument('--debug', help='output debug information', action="store_true")
    #parent_parser.add_argument('--version', help='output version information and quit',  action='version', version=repeatm.__version__)
    parent_parser.add_argument('--quiet', help='only output errors', action="store_true")

    parent_parser.add_argument('--singlem-db', help='sqlite file from a singlem db', required=True)
    parent_parser.add_argument('--condensed-otu-table', help='condensed OTU table input', required=True)
    parent_parser.add_argument('-o', help='sqlite3 file to generate', required=True)
    parent_parser.add_argument('--metadata-json-files', nargs='+', help='metadata json files', required=True)

    args = parent_parser.parse_args()

    # Setup logging
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    # Copy file over
    logging.info("Copying SQLite file")
    sqlite_db_path = args.o
    shutil.copy(args.singlem_db, sqlite_db_path)

    # Create the app and create the tables
    app = create_app()
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + sqlite_db_path
    app.config['SQLALCHEMY_ECHO'] = args.debug is True
    db = SQLAlchemy(app)
    with app.app_context():
        db.create_all()

        # Create all the tables
        Taxonomy.metadata.create_all(db.engine)
        
        fill_condensed_and_taxonomy_tables(db, args.condensed_otu_table)

        fill_metadata(db, args.metadata_json_files)

